{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMS4Z9QH6Cat75sVjkCqqh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergeymasl/my_lifehacks/blob/main/Airflow_base_of_knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cron редактор https://crontab.guru",
        "\n",
        "Книга [Data Pipelines with Apache Airflow](https://disk.yandex.ru/i/oIm9JcyOxEUkNA)\n",
        "\n",
        "[GitHub для книги](https://github.com/BasPH/data-pipelines-with-apache-airflow)"
      ],
      "metadata": {
        "id": "IvbKtW8IyOfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Настройка и установка**"
      ],
      "metadata": {
        "id": "OKQxE03I4ocM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Установка\n",
        "\n",
        "```python\n",
        "# Установка Airflow\n",
        "pip install apache-airflow\n",
        "# apache-airflow==2.1.4\n",
        "```\n",
        "\n",
        "Для установки коннекторов к сторонним сервисам используются \n",
        "```python\n",
        "pip install 'apache-airflow[<PACKAGE>]'\n",
        "```\n",
        "\n",
        "Пример установки\n",
        "```python\n",
        "pip install 'apache-airflow[google,amazon,postgres]'\n",
        "```\n",
        "Также можно установить все пакеты сразу\n",
        "```python\n",
        "pip install 'apache-airflow[all]'\n",
        "```\n",
        "\n",
        "Начиная со 2 версии, все сторонние модули были вынесены в так называемые [провайдеры](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html). Теперь чтобы установить сторонний модуль необходимо исполнить следующую строку (**у меня не сработало**)\n",
        "\n",
        "```python\n",
        "# Установка HTTP провайдера и телеграм провайдера\n",
        "pip install apache-airflow-providers-http\n",
        "pip install apache-airflow-providers-telegram\n",
        "\n",
        "# Библиотека для работы с telegram\n",
        "pip install python-telegram-bot\n",
        "```\n",
        "\n",
        "2. Инициализация\n",
        "\n",
        "На данном этапе мы создаем базу метаданных, от версии к версии команда чутка меняется, это для >2.0\n",
        "\n",
        "```python\n",
        "# Инициализация базы данных\n",
        "airflow db init\n",
        "```\n",
        "При инициализации создается папка с логами, конфиг файл и сама БД. Сама БД может быть любой, под капотом используется ORM, и все зависит лишь от вашего Executor и адреса БД. Которые задаются в глобальных переменных или данных файла конфигурации.\n",
        "\n",
        "Затем можно создать пользователя. Первый пользователь должен иметь роль `Admin`\n",
        "\n",
        "```python\n",
        "# Создадим пользователя Airflow\n",
        "airflow users create \\\n",
        "          --username admin \\\n",
        "          --firstname admin \\\n",
        "          --lastname admin \\\n",
        "          --role Admin \\\n",
        "          --email admin@example.org \\\n",
        "          -p 12345\n",
        "```\n",
        "\n",
        "3. Запуск Веб сервера и шедулера\n",
        "\n",
        "После подготовки, можно запустить вебсервис с мордой на нужном вам порту и шедулер, что это такое мы уже обсуждали ранее. Для них будут созданы отдельные процессы, номера которых будут указаны в файлах рядом с фалом конфигурации. Расширение файлов .pid\n",
        "```python\n",
        "airflow webserver -p 18273 -D\n",
        "airflow scheduler -D\n",
        "```"
      ],
      "metadata": {
        "id": "4U2d5CX64tGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DAG**"
      ],
      "metadata": {
        "id": "-mbocx28j6Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ссылка на документацию [dag](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html?highlight=dag#module-airflow.models.dag)\n",
        "\n",
        ">Примечание `start_date` работает по [**UTC**](https://vremya.org/)\n",
        "\n",
        "А так же свой первый запуск **шедулер** начинает работу по истечению строка указанного в `start_date` (то есть как только наступит следующая минита/час/день после **start_date**\n",
        "\n",
        ">Примечание в `schedule_interval` (`schedule` с версии 2.4) можно передать как **timedelta** так и варажение [cron](https://crontab.guru/)\n",
        "***"
      ],
      "metadata": {
        "id": "yWsG2H2lj-vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Operators**"
      ],
      "metadata": {
        "id": "EmvHkrtRMC4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operators** это задачки которые выполняются в **DAG**\n",
        "\n",
        "[link to a lot of operators](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html)\n",
        "\n",
        "Все операторы строятся на [**BaseOperators**](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html?highlight=baseoperator#airflow.models.baseoperator.BaseOperator)\n",
        "\n",
        "`classairflow.models.baseoperator.BaseOperator(task_id, owner=DEFAULT_OWNER, email=None, email_on_retry=conf.getboolean('email', 'default_email_on_retry', fallback=True), email_on_failure=conf.getboolean('email', 'default_email_on_failure', fallback=True), retries=DEFAULT_RETRIES, retry_delay=DEFAULT_RETRY_DELAY, retry_exponential_backoff=False, max_retry_delay=None, start_date=None, end_date=None, depends_on_past=False, ignore_first_depends_on_past=DEFAULT_IGNORE_FIRST_DEPENDS_ON_PAST, wait_for_downstream=False, dag=None, params=None, default_args=None, priority_weight=DEFAULT_PRIORITY_WEIGHT, weight_rule=DEFAULT_WEIGHT_RULE, queue=DEFAULT_QUEUE, pool=None, pool_slots=DEFAULT_POOL_SLOTS, sla=None, execution_timeout=DEFAULT_TASK_EXECUTION_TIMEOUT, on_execute_callback=None, on_failure_callback=None, on_success_callback=None, on_retry_callback=None, pre_execute=None, post_execute=None, trigger_rule=DEFAULT_TRIGGER_RULE, resources=None, run_as_user=None, task_concurrency=None, max_active_tis_per_dag=None, executor_config=None, do_xcom_push=True, inlets=None, outlets=None, task_group=None, doc=None, doc_md=None, doc_json=None, doc_yaml=None, doc_rst=None, **kwargs)`\n",
        "\n",
        "- `task_id (str)`  - название задачи\n",
        "- `owner (str)`  - владелец задачи\n",
        "- `email (str | Iterable[str] | None) ` - email на который будут приходить оповещения по слудкющим параметрам. Может быть одним адресом или списком адресов\n",
        "- `email_on_retry (bool) ` - отправка оповещения при повторном выполнении задачи\n",
        "- `email_on_failure (bool) ` - отправка оповещения при провале задачи\n",
        "- `retries (int | None)` - количество повторных попыток прежде чем задача будет провалена\n",
        "- `retry_delay (timedelta | float)` - время между повторными попытками, the default is timedelta(seconds=300)\n",
        "- `retry_exponential_backoff (bool) ` - каждый раз увеличивает время ожидания между перезапусками с помощью *Exponential Backoff*\n",
        "- `max_retry_delay (timedelta | float | None)` - максимальное время между попытками при использовании **retry_exponential_backoff**\n",
        "- `start_date (datetime | None)` - дата начала для задачи\n",
        "- `end_date (datetime | None)` - если указана эта дату, планировщик прекратит выполнение задачи в эту дату\n",
        "- `depends_on_past (bool)` - если установлено значение True задача будет выполнена только если предыдущая задача была успешна или пропущена\n",
        "- `wait_for_downstream (bool)` - \n",
        "- `dag (DAG | None)` - даг в котором должна быть задача\n",
        "- `priority_weight (int)` - вес задачи при определении приоритета во время создания резервной копии. Для более важных задач необходимо устанавливать наибольшие значения \n",
        "- `weight_rule (str)` - { `downstream` (defoult)| `upstream` | `absolute` } === дополнить ===\n",
        "- `queue (str)` - === не знаю ===. речь про очереди, пока не встречал\n",
        "- `pool (str | None)` - название **пула** в которых должна выполняться эта задача. С помощью пула можно как ограничить так и увеличить величину потоков для выполнения задачи\n",
        "- `pool_slots (int)` - количество слотов пула которое будет использовать эта задача (должна быть >= 1)\n",
        "- `sla (timedelta | None)`\n",
        "- `execution_timeout (timedelta | None)`\n",
        "- `on_failure_callback (TaskStateChangeCallback | None)` - вызывает указанную **функцию**, при падении\n",
        "- `on_execute_callback (TaskStateChangeCallback | None)` - вызывает указанную **функцию**, перед непосредственным выполением задачи\n",
        "- `on_retry_callback (TaskStateChangeCallback | None)` - вызывает указанную **функцию**, при повторной попытке выполнения задачи\n",
        "- `on_success_callback (TaskStateChangeCallback | None)` - вызывает указанную **функцию**, при успешном выполнении задачи\n",
        "- `pre_execute (TaskPreExecuteHook | None)`\n",
        "- `post_execute (TaskPostExecuteHook | None)`\n",
        "- `trigger_rule (str)` - условия при которых должна выполниться эта задача (описано ниже), если не указана, то будет использована та которая указана в ДАГ \n",
        "- `resources (dict[str, Any] | None)`\n",
        "- `run_as_user (str | None)`\n",
        "- `max_active_tis_per_dag (int | None)`\n",
        "- `executor_config (dict | None)`\n",
        "- `do_xcom_push (bool)`\n",
        "- `task_group (TaskGroup | None)`\n",
        "- `doc (str | None)`\n",
        "- `doc_md (str | None)`\n",
        "- `doc_rst (str | None)`\n",
        "- `doc_json (str | None)`\n",
        "- `doc_yaml (str | None)`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2xJlEj8qq9GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Triger Rules**"
      ],
      "metadata": {
        "id": "2eIeCYoeoXV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Triger rules** - условие при котором начинаются выполняться tasks\n",
        "\n",
        "[link on trigger_rules](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#trigger-rules)\n",
        "\n",
        "`trigger_rule` может быть указан в параметрах `dag` и тогда он будет применен ко всем операторам (если в операторах не будет прописано другое)\n",
        "\n",
        "Также параметр `trigger_rule` для каждого оператора\n",
        "\n",
        "Виды `trigger_rule`:\n",
        "- `all_success` (default) - все `tasks_parents` были **выполены успешно**\n",
        "- `all_failed` - все `tasks_parents` были **провалены**\n",
        "- `all_done` - запускает задачу после `upstream tasks` (`parents') были выполнены **вне зависимости от статуса** успешно, провалено или пропущено \n",
        "- `all_skipped` - все задачи были **пропущены**\n",
        "- `one_failed` - срабатывает **как только одна из задач** была **провалена**\n",
        "- `one_success` - срабатывает **как только одна из задач** была **успешна**\n",
        "- `none_failed` - срабатывает когда задачи `parents` были **успешны** или **пропущены**\n",
        "- `none_failed_min_one_success` - выполняется когда **ни одна из предыдущих** задач **не была провалена**, но **хотя бы одна** была **успешна**\n",
        "- `none_skipped` - срабатывает если не было **пропусков** предыдущих задач\n",
        "- `always` - никаких зависимостей задача может выполниться в любой момент"
      ],
      "metadata": {
        "id": "4xHdw5DAobk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Connections**"
      ],
      "metadata": {
        "id": "mPDBvWGxf_Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connections представляют собой подключения к разным источникам данных, задаются они в веб интерфейсе, с указанием `conn_id`. Это `conn_id` в последствии можно вызвывать в тасках\n",
        "\n",
        "При необходимости к Connections можно получиться как к обычному словарю.\n",
        "\n",
        "[link on connections](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/hooks/base/index.html)\n",
        "\n",
        "```python\n",
        "# Пример доступа к переменной connection\n",
        "from airflow.hooks.base_hook import BaseHook\n",
        "host = BaseHook.get_connection(\"postgres_default\").host\n",
        "pass = BaseHook.get_connection(\"postgres_default\").password\n",
        "```"
      ],
      "metadata": {
        "id": "kPw5sH0vgC-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Variables**"
      ],
      "metadata": {
        "id": "1Boae7t5iUt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Глобальные переменные которые также могут задаваться в веб интерфейсе.\n",
        "\n",
        "Доступ к переменным осуществляется через `key`.\n",
        "Глобальные переменные можно:\n",
        "- получать - `get`\n",
        "- записывать - `set`\n",
        "- обновлять - `update`\n",
        "- удалять - `delete`\n",
        "\n",
        "[link on variables](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/variable/index.html)\n",
        "\n",
        "```python\n",
        "# Пример доступа к глобальной переменной\n",
        "from airflow.models import Variable\n",
        "foo = Variable.get(key = \"key\")\n",
        "# присер записи глобальной переменной\n",
        "a_dict = {'login' : con}\n",
        "Variable.set(key = 'test_key', value = a_dict, serialize_json = True)\n",
        "```"
      ],
      "metadata": {
        "id": "3_ATWn-JibmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sensors**"
      ],
      "metadata": {
        "id": "nIj5_rWErMCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сенсоры - это операторы которые выступает как **указатели наступило ли событие или нет** само действие необходимо прописывать в следующей задаче\n",
        "\n",
        "[link to base_sensors](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.html)\n",
        "\n",
        "[link to a lot of sensors](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.html)\n",
        "\n",
        "Рассмотрим BaseSensorOperator\n",
        "\n",
        "`airflow.sensors.base.BaseSensorOperator(*, poke_interval=60, timeout=conf.getfloat('sensors', 'default_timeout'), \n",
        "soft_fail=False, mode='poke', exponential_backoff=False, **kwargs)`\n",
        "\n",
        "- `poke_interval` (float) - интервал через который будет перезапускаться задача для проверки\n",
        "\n",
        "- `timeout` (float) - время через которое истечет время ожидания и задача будет fail\n",
        "\n",
        "- `soft_fail` (bool) - пропустить задачу при сбое (я не знаю что это значит)\n",
        "\n",
        "- `mode` {'poke', 'reschedule'} - default is 'poke' - \n",
        "> `poke` - сенсор занимает pool на все время до наступления **события**, сенсор переходит в спящий режим между **poke_interval**. Этот режим стоит использовать если работа сенсора предполагается небольшой или интервал проверки нужен небольшойю.\n",
        "\n",
        " > `reschedule` -  Сенсор занимает слот только пока работает потом умирает и тд. Необходимо использовать при большом **poke_interval** (от 1 минуты, иначе будет большая нагрузка на scheduler)\n",
        "\n",
        "- `exponential_backoff` (bool) - каждый раз увеличивает время ожидания между перезапусками с помощью *Exponential Backoff*\n",
        "\n",
        "\n",
        "```python\n",
        "# пример python сенсора оператора\n",
        "partner_b = PythonSensor(\n",
        "    task_id='task',\n",
        "    poke_interval=120, # Через какое время перезапускаться\n",
        "    timeout=10,# Время до принудительного падения\n",
        "    mode=\"reschedule\", # Режим перезапуска\n",
        "    python_callable=func, # функция которая должна вернуть True для выполнения сенсора\n",
        "    soft_fail=True # Пропустить, скипнуть, задачу если она упадет\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "Ah4sRu01rjUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`HttpSensor` - делает запрос по какому либо адресу и при отзыве запускает функцию указанную в параметре `response_check`, в котором нужно проверить полученный ответ и вернуть `True`\n",
        "**response представляет собой класс [requests.response](https://docs-python.ru/packages/modul-requests-python/obekt-otvet-servera-response/)"
      ],
      "metadata": {
        "id": "2vsZjkrr28th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Автогенерация DAG и автогенерация Task**"
      ],
      "metadata": {
        "id": "h0c12iJhkCOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Автогенерация DAG**\n",
        "\n",
        "Для автогенерации DAGs необходимо написать функцию для создания dag и затем в цикле записыать переменные с созданным дагом с использованием `globals()`\n",
        "\n",
        "```python\n",
        "# функция по созданию DAG\n",
        "def create_dag(dag_id, start_date=airflow.utils.dates.days_ago(1), default_args = None, schedule_interval = \"@daily\"):\n",
        "  \n",
        "  # создаем DAG\n",
        "  dag = DAG(dag_id = dag_id, description = f\"This is a auto generated DAG\",\n",
        "  default_args=default_args, start_date=start_date, \n",
        "  schedule_interval=schedule_interval)\n",
        "  \n",
        "  with dag:\n",
        "    # task для дага\n",
        "    t1 = DummyOperator(task_id = \"task_0\", dag = dag)\n",
        "  \n",
        "  return dag\n",
        "\n",
        "# генерация\n",
        "for iDag in range(5):\n",
        "  globals()[f\"dag_{iDag}\"] = create_dag(dag_id = f\"dag_{iDag}\")\n",
        "\n",
        "```\n",
        "\n",
        "> **Автогенерация Task**\n",
        "\n",
        "Для автогенерации task в необходимом даге создается список и в него добавляются все сгенерированные таски\n",
        "```python\n",
        "dag = DAG(dag_id = dag_id, default_args=default_args, start_date=start_date, schedule_interval=schedule_interval)\n",
        "with dag:\n",
        "  # генерация task\n",
        "  for iTask in range(10):\n",
        "    # сама задача\n",
        "    task_list.append(DummyOperator(task_id = f\"task_{iTask}\", dag = dag))\n",
        "    # выстараивание очередности\n",
        "    if iTask > 0:\n",
        "      task_list[iTask - 1] >> task_list[iTask]\n",
        "```"
      ],
      "metadata": {
        "id": "wfnhCMxBkIYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Branch operator**"
      ],
      "metadata": {
        "id": "oGBE5EYsx6dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **context**"
      ],
      "metadata": {
        "id": "Xp9-a84j3JZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ShortCircuitOperator"
      ],
      "metadata": {
        "id": "0H34yuvuq6Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст про shortCircuitOperator"
      ],
      "metadata": {
        "id": "5h6gFWOIq8Yr"
      }
    }
  ]
}
